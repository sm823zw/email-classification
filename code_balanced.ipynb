{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_text_clean(x):\n",
    "    # first we lowercase everything\n",
    "    x = x.lower()\n",
    "    # remove unicode characters\n",
    "    x = x.encode('ascii', 'ignore').decode()\n",
    "    x = re.sub(r'https*\\S+', ' ', x)\n",
    "    x = re.sub(r'http*\\S+', ' ', x)\n",
    "    # then use regex to remove @ symbols and hashtags\n",
    "    #x = re.sub(r'@\\S', '', x)\n",
    "    #x = re.sub(r'#\\S+', ' ', x)\n",
    "    x = re.sub(r'\\'\\w+', '', x)\n",
    "    #x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "    x = re.sub(r'\\w*\\d+\\w*', '', x)\n",
    "    x = re.sub(r'\\s{2,}', ' ', x)\n",
    "    x = re.sub(r'\\s[^\\w\\s]\\s', '', x)\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = nltk.word_tokenize(x)\n",
    "\n",
    "    # Remove stop words\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "    # Join the filtered words back into a sentence\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train, val, and test data\n",
    "train_data = pd.read_csv('new_train_data.csv')\n",
    "val_data = pd.read_csv('val_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes sent by: jeff dasovich make sense for me t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>we need another request for confidentiality.  ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>richard shapiro \\tenron energy services i part...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vance meyer this is the demonstration procedur...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>minor changes shown on the attached (gotta get...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4232</th>\n",
       "      <td>Congratulations on your appointment by fox. th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4233</th>\n",
       "      <td>i check. melissa loves to talk about houston r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>thank you... i think to see your greeting, jus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4235</th>\n",
       "      <td>enjoy! enron capital &amp; trade resources corp. k...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4236</th>\n",
       "      <td>enron capital &amp; trade resources corp. a few mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4232 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     yes sent by: jeff dasovich make sense for me t...      3\n",
       "1     we need another request for confidentiality.  ...      0\n",
       "2     richard shapiro \\tenron energy services i part...      1\n",
       "3     vance meyer this is the demonstration procedur...      5\n",
       "4     minor changes shown on the attached (gotta get...      5\n",
       "...                                                 ...    ...\n",
       "4232  Congratulations on your appointment by fox. th...      1\n",
       "4233  i check. melissa loves to talk about houston r...      1\n",
       "4234  thank you... i think to see your greeting, jus...      1\n",
       "4235  enjoy! enron capital & trade resources corp. k...      1\n",
       "4236  enron capital & trade resources corp. a few mo...      1\n",
       "\n",
       "[4232 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dropna(inplace=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further clean the texts\n",
    "train_data['cleaned_text'] = train_data['text'].apply(simple_text_clean)\n",
    "val_data['cleaned_text'] = val_data['text'].apply(simple_text_clean)\n",
    "test_data['cleaned_text'] = test_data['text'].apply(simple_text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize and preprocess the text data. Max length of a email text is set to 256.\n",
    "def tokenize_text(text, max_length=256):\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, truncation=True)\n",
    "    \n",
    "    # Pad or truncate the tokens to the specified max_length\n",
    "    if len(tokens) < max_length:\n",
    "        # Pad with [PAD] tokens\n",
    "        tokens += [tokenizer.pad_token_id] * (max_length - len(tokens))\n",
    "    elif len(tokens) > max_length:\n",
    "        # Truncate to max_length\n",
    "        tokens = tokens[:max_length]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "train_data['input_ids'] = train_data['cleaned_text'].apply(tokenize_text)\n",
    "val_data['input_ids'] = val_data['cleaned_text'].apply(tokenize_text)\n",
    "test_data['input_ids'] = test_data['cleaned_text'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/Saankhya.Subrata.Mondal/miniforge3/envs/py39_native/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_inputs = torch.tensor(train_data['input_ids'].tolist())\n",
    "val_inputs = torch.tensor(val_data['input_ids'].tolist())\n",
    "test_inputs = torch.tensor(test_data['input_ids'].tolist())\n",
    "\n",
    "train_labels = torch.tensor(train_data['label'].tolist())\n",
    "val_labels = torch.tensor(val_data['label'].tolist())\n",
    "test_labels = torch.tensor(test_data['label'].tolist())\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "val_dataset = TensorDataset(val_inputs, val_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize and fine-tune a BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'mps'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [05:23<00:00,  2.43s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:03<00:00,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.59377121925354, Val Loss: 1.3221588730812073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_predictions = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=f\"Validation\"):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs, labels = batch\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "            logits = outputs.logits\n",
    "            predicted_labels = torch.argmax(logits, dim=1)\n",
    "            val_predictions.extend(predicted_labels.cpu().numpy())\n",
    "            val_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss/len(train_dataloader)}, Val Loss: {val_loss/len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:03<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.64      0.70        83\n",
      "           1       0.33      0.25      0.29         4\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.56      0.49      0.52        47\n",
      "           4       0.17      0.25      0.20         8\n",
      "           5       0.29      0.79      0.42        14\n",
      "\n",
      "    accuracy                           0.54       166\n",
      "   macro avg       0.35      0.40      0.36       166\n",
      "weighted avg       0.59      0.54      0.55       166\n",
      "\n",
      "Confusion Matrix:\n",
      "[[53  0  1  9  5 15]\n",
      " [ 1  1  0  1  1  0]\n",
      " [ 4  0  0  4  1  1]\n",
      " [ 6  2  3 23  3 10]\n",
      " [ 3  0  0  2  2  1]\n",
      " [ 1  0  0  2  0 11]]\n",
      "\n",
      "Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.49      0.57        84\n",
      "           1       0.33      0.33      0.33         3\n",
      "           2       0.33      0.10      0.15        10\n",
      "           3       0.47      0.38      0.42        48\n",
      "           4       0.16      0.43      0.23         7\n",
      "           5       0.14      0.40      0.20        15\n",
      "\n",
      "    accuracy                           0.42       167\n",
      "   macro avg       0.35      0.35      0.32       167\n",
      "weighted avg       0.52      0.42      0.45       167\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  1  0 15  4 23]\n",
      " [ 0  1  0  0  2  0]\n",
      " [ 1  0  1  3  4  1]\n",
      " [ 9  1  1 18  6 13]\n",
      " [ 1  0  0  2  3  1]\n",
      " [ 8  0  1  0  0  6]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "test_targets = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs, labels = batch\n",
    "        outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        test_predictions.extend(predicted_labels.cpu().numpy())\n",
    "        test_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert predictions and targets to NumPy arrays\n",
    "val_predictions = np.array(val_predictions)\n",
    "val_targets = np.array(val_targets)\n",
    "test_predictions = np.array(test_predictions)\n",
    "test_targets = np.array(test_targets)\n",
    "\n",
    "# Print classification report and confusion matrix for validation set\n",
    "print(\"Validation Set:\")\n",
    "print(classification_report(val_targets, val_predictions))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(val_targets, val_predictions))\n",
    "\n",
    "# Print classification report and confusion matrix for test set\n",
    "print(\"\\nTest Set:\")\n",
    "print(classification_report(test_targets, test_predictions))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(test_targets, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39_native')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55605e6521e3f9193fe8f338668c37947d3ff2dbb1f9a55b107405982fa03566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
